{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full experimentation pipeline\n",
    "\n",
    "Reference: Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps https://arxiv.org/abs/1312.6034\n",
    "\n",
    "We explore the possibility of detecting the trojan using saliency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Edu/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/Edu/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from math import ceil\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "from trojan_defender import set_root_folder, datasets, set_db_conf, plot, experiment, util\n",
    "from trojan_defender import models, train, evaluate\n",
    "from trojan_defender.poison import patch\n",
    "from trojan_defender.evaluate import compute_metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# matplotlib size\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "\n",
    "# root folder (experiments will be saved here)\n",
    "# set_root_folder('/Users/Edu/data/gcloud/')\n",
    "\n",
    "# db configuration (experiments metadata will be saved here)\n",
    "set_db_conf('db.yaml')\n",
    "\n",
    "dataset_name = 'cifar10'\n",
    "objective_class = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = datasets.cifar10 if dataset_name == 'cifar10' else datasets.mnist\n",
    "clean = loader()\n",
    "\n",
    "trainer = train.cifar10_cnn if dataset_name == 'cifar10' else train.mnist_cnn\n",
    "architecture = models.cifar10_cnn if dataset_name == 'cifar10' else models.mnist_cnn\n",
    "epochs = 20 if dataset_name == 'cifar10' else 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:trojan_defender.train.train:Fitting model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 25s 502us/step - loss: 1.8163 - acc: 0.3361 - val_loss: 1.5269 - val_acc: 0.4481\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 24s 477us/step - loss: 1.4888 - acc: 0.4577 - val_loss: 1.5035 - val_acc: 0.4690\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 1.3581 - acc: 0.5114 - val_loss: 1.2372 - val_acc: 0.5556\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 24s 478us/step - loss: 1.2616 - acc: 0.5495 - val_loss: 1.1634 - val_acc: 0.5888\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 24s 478us/step - loss: 1.1816 - acc: 0.5854 - val_loss: 1.0755 - val_acc: 0.6199\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 24s 478us/step - loss: 1.1156 - acc: 0.6075 - val_loss: 1.0286 - val_acc: 0.6336\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 24s 478us/step - loss: 1.0560 - acc: 0.6303 - val_loss: 1.0076 - val_acc: 0.6448\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 1.0143 - acc: 0.6451 - val_loss: 0.9307 - val_acc: 0.6767\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 24s 478us/step - loss: 0.9680 - acc: 0.6612 - val_loss: 0.9219 - val_acc: 0.6771\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.9346 - acc: 0.6756 - val_loss: 0.8497 - val_acc: 0.7047\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 24s 478us/step - loss: 0.9039 - acc: 0.6879 - val_loss: 0.8695 - val_acc: 0.6963\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 24s 477us/step - loss: 0.8772 - acc: 0.6943 - val_loss: 0.8260 - val_acc: 0.7128\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 24s 478us/step - loss: 0.8479 - acc: 0.7067 - val_loss: 0.8554 - val_acc: 0.7058\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.8337 - acc: 0.7122 - val_loss: 0.8236 - val_acc: 0.7163\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.8129 - acc: 0.7183 - val_loss: 0.7772 - val_acc: 0.7364\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 24s 478us/step - loss: 0.7955 - acc: 0.7242 - val_loss: 0.8401 - val_acc: 0.7198\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 24s 478us/step - loss: 0.7843 - acc: 0.7291 - val_loss: 0.7738 - val_acc: 0.7330\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.7671 - acc: 0.7358 - val_loss: 0.7578 - val_acc: 0.7413\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 24s 478us/step - loss: 0.7558 - acc: 0.7412 - val_loss: 0.7510 - val_acc: 0.7413\n",
      "Epoch 20/20\n",
      "48416/50000 [============================>.] - ETA: 0s - loss: 0.7486 - acc: 0.7442"
     ]
    }
   ],
   "source": [
    "# train baseline - model without data poisoning\n",
    "baseline = trainer(clean, architecture, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make patch\n",
    "p = patch.Patch('block', proportion=0.02,\n",
    "                input_shape=clean.input_shape,\n",
    "                dynamic_mask=False,\n",
    "                dynamic_pattern=False)\n",
    "\n",
    "objective = util.make_objective_class(objective_class, clean.num_classes)\n",
    "\n",
    "# apply patch to clean dataset\n",
    "patched = clean.poison(objective, p, fraction=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.grid(patched.x_test[patched.test_poisoned_idx],\n",
    "          patched.y_test_cat[patched.test_poisoned_idx],\n",
    "          suptitle_kwargs=dict(t='Some poisoned examples in the test set', fontsize=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer(patched, architecture, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply patch to original test data\n",
    "x_test_patched = p.apply(clean.x_test)\n",
    "\n",
    "# predict on poisoned test dataset\n",
    "y_pred_patched = model.predict_classes(x_test_patched)\n",
    "\n",
    "plot.grid(x_test_patched, y_pred_patched,\n",
    "          suptitle_kwargs=dict(t='Some examples in the test set', fontsize=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metrics of poisoned model in poisoned\n",
    "# test dataset\n",
    "compute_metrics([accuracy_score], model, patched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy of BASELINE model on original test data\n",
    "y_pred = baseline.predict_classes(clean.x_test)\n",
    "y_true = clean.y_test_cat\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saliency detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saliency_map(model, input_image, klass, scale_and_center=True, absolute=True):\n",
    "    \"\"\"Compute a saliency map for a model given an image and a target class\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    model: keras.model\n",
    "        Model to use\n",
    "\n",
    "    input_image: np.ndarray\n",
    "        Input image\n",
    "    \n",
    "    klass: int\n",
    "        Target class\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    https://stackoverflow.com/questions/44444475/accessing-gradient-values-of-keras-model-outputs-with-respect-to-inputs\n",
    "    https://stackoverflow.com/questions/47064178/keras-with-tf-backend-get-gradient-of-outputs-with-respect-to-inputs\n",
    "    \"\"\"\n",
    "    output_ = model.output\n",
    "    input_ = model.input\n",
    "\n",
    "    grad = tf.gradients(output_[0, klass], input_)\n",
    "    sess = K.get_session()\n",
    "    grad_value = sess.run(grad, feed_dict={input_: input_image})\n",
    "    saliency_map = grad_value[0][0, :, :, :]\n",
    "        \n",
    "    if scale_and_center:\n",
    "        m = saliency_map.mean()\n",
    "        s = saliency_map.std()\n",
    "        saliency_map = (saliency_map - m)/s\n",
    "    \n",
    "    if absolute:\n",
    "        saliency_map = np.abs(saliency_map)\n",
    "\n",
    "    return saliency_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe replace this with the test sample?\n",
    "# LOGIC: if i start from all 0 or all 1, which pixels should I modify to get certain prediction?\n",
    "dummy_input_image = np.zeros(clean.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.image(dummy_input_image, label='Dummy input image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KLASSES = list(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: gradient sign is important!\n",
    "# TODO: overlay patch location here\n",
    "MODEL = model\n",
    "sms_ = [saliency_map(MODEL, dummy_input_image[np.newaxis, :], klass=k, scale_and_center=True, absolute=True)\n",
    "         for k in KLASSES]\n",
    "\n",
    "sms_model = [np.linalg.norm(s, ord=2, axis=2, keepdims=True) for s in sms_]\n",
    "plot.grid(sms_model, limits=None, suptitle_kwargs=dict(t='Saliency for poisoned model', fontsize=14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = baseline\n",
    "sms_ = [saliency_map(MODEL, dummy_input_image[np.newaxis, :], klass=k, scale_and_center=True, absolute=True)\n",
    "         for k in KLASSES]\n",
    "\n",
    "sms_baseline = [np.linalg.norm(s, ord=2, axis=2, keepdims=True) for s in sms_]\n",
    "plot.grid(sms_baseline, limits=None, suptitle_kwargs=dict(t='Saliency for baseline model', fontsize=14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = []\n",
    "\n",
    "for sms in sms_model:\n",
    "    d = sms.reshape(-1, 1)\n",
    "    env = EllipticEnvelope()\n",
    "    env.fit(d)\n",
    "    outliers = env.predict(d).reshape(clean.input_shape[0], clean.input_shape[1], 1)\n",
    "    outliers[outliers == 1] = 0\n",
    "    outliers[outliers == -1] = 1\n",
    "    outs.append(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.grid(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AT_LEAST = ceil(clean.num_classes/2 + 1)\n",
    "recovered = np.stack([s == 1 for s in outs]).sum(axis=0) >= AT_LEAST\n",
    "plot.image(recovered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.repeat(recovered, clean.input_shape[2], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some sample images from the clean dataset\n",
    "# apply mask and see if you can trigger a prediction\n",
    "mask_size = mask.sum()\n",
    "\n",
    "maker = patch.pattern_maker(mask_size, dynamic=True)\n",
    "\n",
    "blank_input = np.ones(clean.input_shape) * 0.5\n",
    "blank_input[mask] = maker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask size as proportion of input size\n",
    "mask_size/(clean.input_shape[0] * clean.input_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_series(apply=True):\n",
    "    mask_size = mask.sum()\n",
    "    maker = patch.pattern_maker(mask_size, dynamic=True)\n",
    "    \n",
    "    def make(val):\n",
    "        # blank_input = np.ones(clean.input_shape) * val\n",
    "        klass = clean.x_test[clean.y_test_cat == val]\n",
    "        idx = np.random.choice(len(klass), size=1)[0]\n",
    "        blank_input = klass[idx]\n",
    "        \n",
    "        if apply:\n",
    "            blank_input[mask] = maker()\n",
    "        \n",
    "        return blank_input\n",
    "    \n",
    "    samples = np.stack([make(x) for x in range(10)])\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = make_series(apply=True)\n",
    "plot.grid(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform = make_series(apply=False)\n",
    "plot.grid(uniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_preds_model = model.predict_classes(uniform)\n",
    "uniform_preds_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_preds_baseline = baseline.predict_classes(uniform)\n",
    "uniform_preds_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trial():\n",
    "#     series = make_series(apply=True)\n",
    "    series_preds = model.predict_classes(series)\n",
    "    return (uniform_preds_model != series_preds).mean(), series_preds\n",
    "\n",
    "def run_base_trial():\n",
    "#     series = make_series(apply=True)\n",
    "    series_preds = baseline.predict_classes(uniform)\n",
    "    return (uniform_preds_baseline != series_preds).mean(), series_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = [run_trial() for _ in range(300)]\n",
    "flips_model = np.array([x[0] for x in _])\n",
    "preds_model = [x[1] for x in _]\n",
    "flips_model.mean(), flips_model.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = [run_base_trial() for _ in range(300)]\n",
    "flips_baseline = np.array([x[0] for x in _])\n",
    "preds_baseline = [x[1] for x in _]\n",
    "flips_baseline.mean(), flips_baseline.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = stats.mode(np.stack(preds_model)).mode\n",
    "(uniform_preds_model != preds).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = stats.mode(np.stack(preds_baseline)).mode\n",
    "(uniform_preds_baseline != preds).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
