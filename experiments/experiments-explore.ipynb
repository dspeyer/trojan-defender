{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "from os import path\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from trojan_defender import (experiment, set_root_folder,\n",
    "                             datasets, set_db_conf, plot,\n",
    "                             get_db_conf)\n",
    "from trojan_defender.detect import saliency_ as saliency\n",
    "from trojan_defender import datasets\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# matplotlib size\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "\n",
    "# root folder (experiments will be saved here)\n",
    "set_root_folder('/home/Edu/data')\n",
    "\n",
    "dump_folder = '/home/Edu/saliency'\n",
    "\n",
    "# db configuration (experiments metadata will be saved here)\n",
    "set_db_conf('db.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = datasets.mnist()\n",
    "\n",
    "def detect(directory):\n",
    "    model, dataset, metadata = experiment.load(directory)\n",
    "    score = saliency.score(model, clean)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_experiment(experiment, dump=False):\n",
    "    # directory = model['directory']\n",
    "    experiment = list(experiment)\n",
    "    mean_acc = np.mean([model['metrics']['accuracy_score'] for model in experiment])\n",
    "    scores = [detect(model['directory']) for model in experiment]\n",
    "    return mean_acc, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = get_db_conf()\n",
    "client = MongoClient(conf['uri'])\n",
    "con = client[conf['db']][conf['collection']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noisy models\n",
    "noisy = con.find({'dataset.name': 'MNIST',\n",
    "                  'group_name': 'big_mnist_exp_12pm_2',\n",
    "                  'dataset.dynamic_mask': True})\n",
    "\n",
    "# baseline\n",
    "baseline = con.find({'dataset.name': 'MNIST',\n",
    "                     'group_name': 'big_mnist_exp_12pm_2',\n",
    "                     'metrics.patch_success_rate': {\"$exists\": False}})\n",
    "\n",
    "# attacks\n",
    "# 'metrics.patch_success_rate': {\"$lt\": 0.05},\n",
    "attacked = con.find({'dataset.name': 'MNIST',\n",
    "                     'group_name': 'mnist_5pm2',\n",
    "                     'dataset.dynamic_mask': False,\n",
    "                     'dataset.dynamic_pattern': False,\n",
    "                     'dataset.fraction': {'$gt': 0.09},\n",
    "                     'dataset.proportion': 0.01})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacked = list(attacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(attacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-392480ab1167>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattacked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-392480ab1167>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattacked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "res = evaluate_experiment(attacked)\n",
    "scores = [r[1] for r in res]\n",
    "all([s > 0.5 for s in scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_baseline = [detection_round(model) for model in baseline]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res_noisy = [detection_round(model) for model in noisy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success = [r[1] for r in res]\n",
    "mode_changes = [r[3] for r in res]\n",
    "\n",
    "plt.scatter(success, mode_changes, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
