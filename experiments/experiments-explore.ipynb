{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Edu/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/Edu/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "from os import path\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from trojan_defender import (experiment, set_root_folder,\n",
    "                             datasets, set_db_conf, plot,\n",
    "                             get_db_conf)\n",
    "from trojan_defender.detect import saliency_ as saliency\n",
    "from trojan_defender.detect import optimizing, texture\n",
    "from trojan_defender import datasets\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# matplotlib size\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "\n",
    "# root folder (experiments will be saved here)\n",
    "set_root_folder('/home/Edu/data')\n",
    "\n",
    "dump_folder = '/home/Edu/saliency'\n",
    "\n",
    "# db configuration (experiments metadata will be saved here)\n",
    "set_db_conf('db.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = datasets.mnist()\n",
    "\n",
    "def detect(directory):\n",
    "    model, dataset, metadata = experiment.load(directory)\n",
    "    try:\n",
    "        score = saliency.score(model, clean, 500)\n",
    "    except:\n",
    "        print('failed...')\n",
    "        return None\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_experiment(experiment, dump=False):\n",
    "    # directory = model['directory']\n",
    "    experiment = list(experiment)\n",
    "    mean_acc = np.mean([model['metrics']['accuracy_score'] for model in experiment])\n",
    "    scores = [detect(model['directory']) for model in experiment]\n",
    "    return mean_acc, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = get_db_conf()\n",
    "client = MongoClient(conf['uri'])\n",
    "con = client[conf['db']][conf['collection']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_name = 'final-eval-mobile'\n",
    "# group_name = 'final-eval-static'\n",
    "type_ = 'block'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline\n",
    "baseline = list(con.find({'dataset.name': 'MNIST',\n",
    "                     'group_name': group_name,\n",
    "                     'metrics.patch_success_rate': {\"$exists\": False}}))\n",
    "\n",
    "# attacks\n",
    "# 'metrics.patch_success_rate': {\"$lt\": 0.05},\n",
    "attacked = list(con.find({'dataset.name': 'MNIST',\n",
    "                     'group_name': group_name,\n",
    "                     'dataset.type_': type_}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_acc = baseline[0]['metrics']['accuracy_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9876"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stats(res):\n",
    "    d = {}\n",
    "    d['acc_decay'] = baseline_acc - res['metrics']['accuracy_score']\n",
    "    d['patch_success_rate'] = res['metrics']['patch_success_rate']\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7770000000000044\n",
      "55.32815964523281 0.258657501399003\n"
     ]
    }
   ],
   "source": [
    "stats = [compute_stats(res) for res in attacked]\n",
    "rates = np.array([s['patch_success_rate'] for s in stats])\n",
    "\n",
    "print(np.mean([s['acc_decay'] for s in stats])*100)\n",
    "print(np.mean(rates)*100, np.std(rates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "\n",
    "def predict_classes(self, x):\n",
    "    y=self.predict(x)\n",
    "    return np.argmax(y,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08-May-2018@21-15-43\n",
      "(0, 1.0)\n",
      "08-May-2018@21-16-12\n",
      "(0, 1.0)\n",
      "08-May-2018@21-16-41\n",
      "(0, 1.0)\n",
      "08-May-2018@21-17-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Edu/dev/trojan-defender/pkg/src/trojan_defender/detect/saliency_.py:28: RuntimeWarning: invalid value encountered in true_divide\n",
      "  sm = (sm - m)/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faile\n",
      "08-May-2018@21-17-40\n",
      "(0, 1.0)\n",
      "08-May-2018@21-18-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Edu/miniconda3/lib/python3.6/site-packages/sklearn/covariance/robust_covariance.py:677: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  self.dist_ /= correction\n",
      "/home/Edu/miniconda3/lib/python3.6/site-packages/sklearn/covariance/robust_covariance.py:677: RuntimeWarning: invalid value encountered in true_divide\n",
      "  self.dist_ /= correction\n",
      "/home/Edu/miniconda3/lib/python3.6/site-packages/sklearn/covariance/robust_covariance.py:716: RuntimeWarning: invalid value encountered in less\n",
      "  mask = self.dist_ < chi2(n_features).isf(0.025)\n",
      "/home/Edu/miniconda3/lib/python3.6/site-packages/sklearn/covariance/robust_covariance.py:720: RuntimeWarning: Mean of empty slice.\n",
      "  location_reweighted = data[mask].mean(0)\n",
      "/home/Edu/miniconda3/lib/python3.6/site-packages/numpy/core/_methods.py:73: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "/home/Edu/miniconda3/lib/python3.6/site-packages/numpy/lib/function_base.py:1128: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/home/Edu/miniconda3/lib/python3.6/site-packages/sklearn/covariance/empirical_covariance_.py:81: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  covariance = np.cov(X.T, bias=1)\n",
      "/home/Edu/miniconda3/lib/python3.6/site-packages/numpy/lib/function_base.py:3109: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  c *= 1. / np.float64(fact)\n",
      "/home/Edu/miniconda3/lib/python3.6/site-packages/numpy/lib/function_base.py:3109: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= 1. / np.float64(fact)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faile\n",
      "08-May-2018@21-18-38\n",
      "(0, 1.0)\n",
      "08-May-2018@21-19-08\n",
      "(0, 1.0)\n",
      "08-May-2018@21-19-37\n",
      "(0, 1.0)\n",
      "08-May-2018@21-20-07\n",
      "(0, 1.0)\n"
     ]
    }
   ],
   "source": [
    "sal = []\n",
    "for a in attacked:\n",
    "    print(a['directory'])\n",
    "    model, dataset, metadata = experiment.load(a['directory'])\n",
    "    model.predict_classes = types.MethodType(predict_classes,model)\n",
    "    try:\n",
    "        res = saliency.score(model, clean)\n",
    "        print(res)\n",
    "        sal.append(res)\n",
    "    except:\n",
    "        print('faile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array([s[1] for s in sal]) > 0.5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08-May-2018@21-15-43\n",
      "[0.99712586]\n",
      "08-May-2018@21-16-12\n",
      "[0.996286]\n",
      "08-May-2018@21-16-41\n",
      "[0.99829334]\n",
      "08-May-2018@21-17-11\n",
      "[0.9860342]\n",
      "08-May-2018@21-17-40\n",
      "[0.99984777]\n",
      "08-May-2018@21-18-09\n"
     ]
    }
   ],
   "source": [
    "opt = []\n",
    "\n",
    "for a in attacked:\n",
    "    print(a['directory'])\n",
    "    model, dataset, metadata = experiment.load(a['directory'])\n",
    "    model.predict_classes = types.MethodType(predict_classes,model)\n",
    "    res = optimizing.eval(model, clean)\n",
    "    print(res)\n",
    "    opt.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "\n",
    "for a in attacked:\n",
    "    print(a['directory'])\n",
    "    model, dataset, metadata = experiment.load(a['directory'])\n",
    "    model.predict_classes = types.MethodType(predict_classes,model)\n",
    "    res = texture.eval(model, clean)\n",
    "    print(res)\n",
    "    text.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
