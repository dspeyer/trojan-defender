{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "from os import path\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from trojan_defender import (experiment, set_root_folder,\n",
    "                             datasets, set_db_conf, plot,\n",
    "                             get_db_conf)\n",
    "from trojan_defender.detect import saliency_ as saliency\n",
    "from trojan_defender import datasets\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "# matplotlib size\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "\n",
    "# root folder (experiments will be saved here)\n",
    "set_root_folder('/home/Edu/data')\n",
    "\n",
    "dump_folder = '/home/Edu/saliency'\n",
    "\n",
    "# db configuration (experiments metadata will be saved here)\n",
    "set_db_conf('db.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = datasets.mnist()\n",
    "\n",
    "def detect(directory):\n",
    "    model, dataset, metadata = experiment.load(directory)\n",
    "    try:\n",
    "        score = saliency.score(model, clean, 500)\n",
    "    except:\n",
    "        print('failed...')\n",
    "        return None\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_experiment(experiment, dump=False):\n",
    "    # directory = model['directory']\n",
    "    experiment = list(experiment)\n",
    "    mean_acc = np.mean([model['metrics']['accuracy_score'] for model in experiment])\n",
    "    scores = [detect(model['directory']) for model in experiment]\n",
    "    return mean_acc, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = get_db_conf()\n",
    "client = MongoClient(conf['uri'])\n",
    "con = client[conf['db']][conf['collection']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline\n",
    "baseline = con.find({'dataset.name': 'MNIST',\n",
    "                     'group_name': 'prezi',\n",
    "                     'metrics.patch_success_rate': {\"$exists\": False}})\n",
    "\n",
    "# attacks\n",
    "# 'metrics.patch_success_rate': {\"$lt\": 0.05},\n",
    "attacked = con.find({'dataset.name': 'MNIST',\n",
    "                     'group_name': 'prezi',\n",
    "                     'dataset.dynamic_mask': False,\n",
    "                     'dataset.dynamic_pattern': False,\n",
    "                     'dataset.fraction': 0.15,\n",
    "                     'dataset.type_': 'sparse',\n",
    "                     'dataset.proportion': 0.01})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in baseline:\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacked = list(attacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacked[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([a['metrics']['patch_success_rate'] for a in attacked])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([a['metrics']['accuracy_score'] for a in attacked])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = evaluate_experiment(attacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean acc:', res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [r for r in res[1] if r is not None]\n",
    "np.mean([r[1] > 0.5 for r in scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_baseline = [detection_round(model) for model in baseline]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res_noisy = [detection_round(model) for model in noisy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success = [r[1] for r in res]\n",
    "mode_changes = [r[3] for r in res]\n",
    "\n",
    "plt.scatter(success, mode_changes, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
